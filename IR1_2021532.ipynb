{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Data Preprocessing"
      ],
      "metadata": {
        "id": "8ALIj107Mo2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UnZNPgKMZxM",
        "outputId": "0b0b47df-c5b2-4635-a1bc-6d5d857677e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJQuyeztSNom",
        "outputId": "d69853c5-f7f0-41f8-bbf9-61d9110d74be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s51alAlRJPaD",
        "outputId": "d64c28f4-cb13-4ac2-8293-429c54d34312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing file: file229.txt\n",
            "\n",
            "Original text:\n",
            "-----\n",
            "This is a beautiful capo, and with the action on my guitar I can use it anywhere after moving it a bit, a five star product ...\n",
            "-----\n",
            "\n",
            "After removing HTML tags:\n",
            "-----\n",
            "this is a beautiful capo, and with the action on my guitar i can use it anywhere after moving it a bit, a five star product ...\n",
            "-----\n",
            "\n",
            "After tokenization:\n",
            "-----\n",
            "['this', 'is', 'a', 'beautiful', 'capo', ',', 'and', 'with', 'the', 'action', 'on', 'my', 'guitar', 'i', 'can', 'use', 'it', 'anywhere', 'after', 'moving', 'it', 'a', 'bit', ',', 'a', 'five', 'star', 'product'] ...\n",
            "-----\n",
            "\n",
            "After removing stop words:\n",
            "-----\n",
            "['beautiful', 'capo', ',', 'action', 'guitar', 'use', 'anywhere', 'moving', 'bit', ',', 'five', 'star', 'product'] ...\n",
            "-----\n",
            "\n",
            "After removing non-alphanumeric characters:\n",
            "-----\n",
            "['beautiful', 'capo', 'action', 'guitar', 'use', 'anywhere', 'moving', 'bit', 'five', 'star', 'product'] ...\n",
            "-----\n",
            "\n",
            "Final preprocessed text:\n",
            "-----\n",
            "beautiful capo action guitar use anywhere moving bit five star product ...\n",
            "-----\n",
            "\n",
            "\n",
            "Processing file: file577.txt\n",
            "\n",
            "Original text:\n",
            "-----\n",
            "Perfect fit for 2001 Mexican Standard Telecaster.  Two protective layers of plastic coating.  Looks great! ...\n",
            "-----\n",
            "\n",
            "After removing HTML tags:\n",
            "-----\n",
            "perfect fit for 2001 mexican standard telecaster.  two protective layers of plastic coating.  looks great! ...\n",
            "-----\n",
            "\n",
            "After tokenization:\n",
            "-----\n",
            "['perfect', 'fit', 'for', '2001', 'mexican', 'standard', 'telecaster', '.', 'two', 'protective', 'layers', 'of', 'plastic', 'coating', '.', 'looks', 'great', '!'] ...\n",
            "-----\n",
            "\n",
            "After removing stop words:\n",
            "-----\n",
            "['perfect', 'fit', '2001', 'mexican', 'standard', 'telecaster', '.', 'two', 'protective', 'layers', 'plastic', 'coating', '.', 'looks', 'great', '!'] ...\n",
            "-----\n",
            "\n",
            "After removing non-alphanumeric characters:\n",
            "-----\n",
            "['perfect', 'fit', '2001', 'mexican', 'standard', 'telecaster', 'two', 'protective', 'layers', 'plastic', 'coating', 'looks', 'great'] ...\n",
            "-----\n",
            "\n",
            "Final preprocessed text:\n",
            "-----\n",
            "perfect fit 2001 mexican standard telecaster two protective layers plastic coating looks great ...\n",
            "-----\n",
            "\n",
            "\n",
            "Processing file: file531.txt\n",
            "\n",
            "Original text:\n",
            "-----\n",
            "Great little mouthpiece sleeve that fits my Otto Link hard rubber tenor sax mouthpiece near-perfectly!\n",
            "\n",
            "Glad to offer my mouthpiece a bit of protection in my case and on the go. It's nothing overly substantial, no thick padding or the likes. But it's great for keeping track of your mouthpiece outsid ...\n",
            "-----\n",
            "\n",
            "After removing HTML tags:\n",
            "-----\n",
            "great little mouthpiece sleeve that fits my otto link hard rubber tenor sax mouthpiece near-perfectly!\n",
            "\n",
            "glad to offer my mouthpiece a bit of protection in my case and on the go. it's nothing overly substantial, no thick padding or the likes. but it's great for keeping track of your mouthpiece outsid ...\n",
            "-----\n",
            "\n",
            "After tokenization:\n",
            "-----\n",
            "['great', 'little', 'mouthpiece', 'sleeve', 'that', 'fits', 'my', 'otto', 'link', 'hard', 'rubber', 'tenor', 'sax', 'mouthpiece', 'near-perfectly', '!', 'glad', 'to', 'offer', 'my', 'mouthpiece', 'a', 'bit', 'of', 'protection', 'in', 'my', 'case', 'and', 'on', 'the', 'go', '.', 'it', \"'s\", 'nothing', 'overly', 'substantial', ',', 'no', 'thick', 'padding', 'or', 'the', 'likes', '.', 'but', 'it', \"'s\", 'great'] ...\n",
            "-----\n",
            "\n",
            "After removing stop words:\n",
            "-----\n",
            "['great', 'little', 'mouthpiece', 'sleeve', 'fits', 'otto', 'link', 'hard', 'rubber', 'tenor', 'sax', 'mouthpiece', 'near-perfectly', '!', 'glad', 'offer', 'mouthpiece', 'bit', 'protection', 'case', 'go', '.', \"'s\", 'nothing', 'overly', 'substantial', ',', 'thick', 'padding', 'likes', '.', \"'s\", 'great', 'keeping', 'track', 'mouthpiece', 'outside', 'case', 'go', '.', \"'re\", 'looking', 'cheap', 'worthwhile', 'case', ',', '.'] ...\n",
            "-----\n",
            "\n",
            "After removing non-alphanumeric characters:\n",
            "-----\n",
            "['great', 'little', 'mouthpiece', 'sleeve', 'fits', 'otto', 'link', 'hard', 'rubber', 'tenor', 'sax', 'mouthpiece', 'glad', 'offer', 'mouthpiece', 'bit', 'protection', 'case', 'go', 'nothing', 'overly', 'substantial', 'thick', 'padding', 'likes', 'great', 'keeping', 'track', 'mouthpiece', 'outside', 'case', 'go', 'looking', 'cheap', 'worthwhile', 'case'] ...\n",
            "-----\n",
            "\n",
            "Final preprocessed text:\n",
            "-----\n",
            "great little mouthpiece sleeve fits otto link hard rubber tenor sax mouthpiece glad offer mouthpiece bit protection case go nothing overly substantial thick padding likes great keeping track mouthpiece outside case go looking cheap worthwhile case ...\n",
            "-----\n",
            "\n",
            "\n",
            "Processing file: file963.txt\n",
            "\n",
            "Original text:\n",
            "-----\n",
            "I  hooked up the light to a karaoke and mixer board very happy and pleased with her performance .  Lights are controlled by a DMX  DJ  Operating board . ...\n",
            "-----\n",
            "\n",
            "After removing HTML tags:\n",
            "-----\n",
            "i  hooked up the light to a karaoke and mixer board very happy and pleased with her performance .  lights are controlled by a dmx  dj  operating board . ...\n",
            "-----\n",
            "\n",
            "After tokenization:\n",
            "-----\n",
            "['i', 'hooked', 'up', 'the', 'light', 'to', 'a', 'karaoke', 'and', 'mixer', 'board', 'very', 'happy', 'and', 'pleased', 'with', 'her', 'performance', '.', 'lights', 'are', 'controlled', 'by', 'a', 'dmx', 'dj', 'operating', 'board', '.'] ...\n",
            "-----\n",
            "\n",
            "After removing stop words:\n",
            "-----\n",
            "['hooked', 'light', 'karaoke', 'mixer', 'board', 'happy', 'pleased', 'performance', '.', 'lights', 'controlled', 'dmx', 'dj', 'operating', 'board', '.'] ...\n",
            "-----\n",
            "\n",
            "After removing non-alphanumeric characters:\n",
            "-----\n",
            "['hooked', 'light', 'karaoke', 'mixer', 'board', 'happy', 'pleased', 'performance', 'lights', 'controlled', 'dmx', 'dj', 'operating', 'board'] ...\n",
            "-----\n",
            "\n",
            "Final preprocessed text:\n",
            "-----\n",
            "hooked light karaoke mixer board happy pleased performance lights controlled dmx dj operating board ...\n",
            "-----\n",
            "\n",
            "\n",
            "Processing file: file741.txt\n",
            "\n",
            "Original text:\n",
            "-----\n",
            "I have 3 daughters and last year I bought my oldest a guitar.  She's been kind of luke warm ever since getting one but my middle daughter has been asking for one for a while now, so this was her birthday gift.  It's what you'd expect it to be....it's a Fender, but it's a Squier...so it's going to be ...\n",
            "-----\n",
            "\n",
            "After removing HTML tags:\n",
            "-----\n",
            "i have 3 daughters and last year i bought my oldest a guitar.  she's been kind of luke warm ever since getting one but my middle daughter has been asking for one for a while now, so this was her birthday gift.  it's what you'd expect it to be....it's a fender, but it's a squier...so it's going to be ...\n",
            "-----\n",
            "\n",
            "After tokenization:\n",
            "-----\n",
            "['i', 'have', '3', 'daughters', 'and', 'last', 'year', 'i', 'bought', 'my', 'oldest', 'a', 'guitar', '.', 'she', \"'s\", 'been', 'kind', 'of', 'luke', 'warm', 'ever', 'since', 'getting', 'one', 'but', 'my', 'middle', 'daughter', 'has', 'been', 'asking', 'for', 'one', 'for', 'a', 'while', 'now', ',', 'so', 'this', 'was', 'her', 'birthday', 'gift', '.', 'it', \"'s\", 'what', 'you'] ...\n",
            "-----\n",
            "\n",
            "After removing stop words:\n",
            "-----\n",
            "['3', 'daughters', 'last', 'year', 'bought', 'oldest', 'guitar', '.', \"'s\", 'kind', 'luke', 'warm', 'ever', 'since', 'getting', 'one', 'middle', 'daughter', 'asking', 'one', ',', 'birthday', 'gift', '.', \"'s\", \"'d\", 'expect', '....', \"'s\", 'fender', ',', \"'s\", 'squier', '...', \"'s\", 'going', 'mass', 'produced', '``', 'alright', \"''\", 'quality', 'part', ',', 'stellar', 'means', '.', 'nevertheless', ',', 'gets'] ...\n",
            "-----\n",
            "\n",
            "After removing non-alphanumeric characters:\n",
            "-----\n",
            "['3', 'daughters', 'last', 'year', 'bought', 'oldest', 'guitar', 'kind', 'luke', 'warm', 'ever', 'since', 'getting', 'one', 'middle', 'daughter', 'asking', 'one', 'birthday', 'gift', 'expect', 'fender', 'squier', 'going', 'mass', 'produced', 'alright', 'quality', 'part', 'stellar', 'means', 'nevertheless', 'gets', 'job', 'done', 'getting', 'someone', 'young', 'interested', 'learning', 'process', 'prepared', 'tighten', 'various', 'screws', 'nuts', 'guitar', 'arrives', 'docked', 'one'] ...\n",
            "-----\n",
            "\n",
            "Final preprocessed text:\n",
            "-----\n",
            "3 daughters last year bought oldest guitar kind luke warm ever since getting one middle daughter asking one birthday gift expect fender squier going mass produced alright quality part stellar means nevertheless gets job done getting someone young interested learning process prepared tighten various  ...\n",
            "-----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    print(\"Original text:\\n-----\")\n",
        "    print(text[:300], \"...\")  # Print the first 300 characters\n",
        "    print(\"-----\\n\")\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove HTML tags using BeautifulSoup\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    text = soup.get_text()\n",
        "    print(\"After removing HTML tags:\\n-----\")\n",
        "    print(text[:300], \"...\")\n",
        "    print(\"-----\\n\")\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    print(\"After tokenization:\\n-----\")\n",
        "    print(tokens[:50], \"...\")  # Print the first 50 tokens\n",
        "    print(\"-----\\n\")\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    print(\"After removing stop words:\\n-----\")\n",
        "    print(tokens[:50], \"...\")  # 50 tokens\n",
        "    print(\"-----\\n\")\n",
        "\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "    print(\"After removing non-alphanumeric characters:\\n-----\")\n",
        "    print(tokens[:50], \"...\")\n",
        "    print(\"-----\\n\")\n",
        "\n",
        "    # Remove blank space tokens\n",
        "    tokens = [word for word in tokens if word.strip()]\n",
        "\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "    print(\"Final preprocessed text:\\n-----\")\n",
        "    print(preprocessed_text[:300], \"...\")  # First 300 characters\n",
        "    print(\"-----\\n\")\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "def preprocess_and_save_files(dataset_path, preprocessed_path):\n",
        "    files = os.listdir(dataset_path)\n",
        "    sample_files = random.sample(files, min(5, len(files)))\n",
        "\n",
        "    for file_name in sample_files:\n",
        "        file_path = os.path.join(dataset_path, file_name)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            original_text = file.read()\n",
        "            print(f\"\\nProcessing file: {file_name}\\n\")\n",
        "            preprocessed_text = preprocess_text(original_text)\n",
        "\n",
        "        preprocessed_file_path = os.path.join(preprocessed_path, f\"preprocessed_{file_name}\")\n",
        "        with open(preprocessed_file_path, 'w', encoding='utf-8') as preprocessed_file:\n",
        "            preprocessed_file.write(preprocessed_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_path = \"/content/drive/My Drive/CSE508_Winter2024_A1_2021532/text_files\"\n",
        "    preprocessed_path = \"/content/drive/My Drive/CSE508_Winter2024_A1_2021532/Preprocessed_files\"\n",
        "\n",
        "    if not os.path.exists(preprocessed_path):\n",
        "        os.makedirs(preprocessed_path)\n",
        "\n",
        "    preprocess_and_save_files(dataset_path, preprocessed_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Unigram Inverted Index and Boolean Queries\n"
      ],
      "metadata": {
        "id": "Hev30RlWSb1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "\n",
        "def create_inverted_index(preprocessed_path):\n",
        "    inverted_index = defaultdict(set)\n",
        "    for file_name in os.listdir(preprocessed_path):\n",
        "        file_path = os.path.join(preprocessed_path, file_name)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            words = file.read().split()\n",
        "            for word in words:\n",
        "                inverted_index[word].add(file_name)\n",
        "    return inverted_index\n",
        "\n",
        "def save_inverted_index(inverted_index, file_name):\n",
        "    with open(file_name, 'wb') as file:\n",
        "        pickle.dump(inverted_index, file)\n",
        "\n",
        "def load_inverted_index(file_name):\n",
        "    with open(file_name, 'rb') as file:\n",
        "        return pickle.load(file)\n",
        "\n",
        "def perform_query(inverted_index, query_terms, operations):\n",
        "    result_sets = [inverted_index.get(term, set()) for term in query_terms]\n",
        "\n",
        "    result = result_sets[0]\n",
        "    for op, next_set in zip(operations, result_sets[1:]):\n",
        "        if op == 'AND':\n",
        "            result = result & next_set\n",
        "        elif op == 'OR':\n",
        "            result = result | next_set\n",
        "        elif op == 'AND NOT':\n",
        "            result = result - next_set\n",
        "        elif op == 'OR NOT':\n",
        "            result = (result | next_set) - next_set\n",
        "    return result\n",
        "\n",
        "def format_query(query_terms, operations):\n",
        "    formatted_query = []\n",
        "    for term, op in zip(query_terms, operations + ['']):\n",
        "        formatted_query.append(term)\n",
        "        if op:\n",
        "            formatted_query.append(op)\n",
        "    return ' '.join(formatted_query)\n",
        "\n",
        "def process_queries(preprocessed_path, inverted_index_file):\n",
        "    inverted_index = load_inverted_index(inverted_index_file)\n",
        "    n = int(input(\"Enter the number of queries: \"))\n",
        "    queries = []\n",
        "    for i in range(n):\n",
        "        query_text = input(f\"Enter query {i + 1}: \")\n",
        "        operations = input(f\"Enter operations for query {i + 1}, separated by commas: \").split(', ')\n",
        "        queries.append((query_text, operations))\n",
        "\n",
        "    for i, (query_text, operations) in enumerate(queries):\n",
        "        preprocessed_query = preprocess_text(query_text)\n",
        "        query_terms = preprocessed_query.split()\n",
        "\n",
        "        result_docs = perform_query(inverted_index, query_terms, operations)\n",
        "        formatted_query = format_query(query_terms, operations)\n",
        "        print(f\"\\nQuery {i + 1}: {formatted_query}\")\n",
        "        print(f\"Number of documents retrieved for query {i + 1}: {len(result_docs)}\")\n",
        "        print(f\"Names of the documents retrieved for query {i + 1}: {', '.join(result_docs)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_path = \"/content/drive/My Drive/CSE508_Winter2024_A1_2021532/text_files\"\n",
        "    preprocessed_path = \"/content/drive/My Drive/CSE508_Winter2024_A1_2021532/Preprocessed_files\"\n",
        "    inverted_index_file = \"/content/drive/My Drive/CSE508_Winter2024_A1_2021532/inverted_index.pkl\"\n",
        "\n",
        "    preprocess_and_save_files(dataset_path, preprocessed_path)\n",
        "\n",
        "    inverted_index = create_inverted_index(preprocessed_path)\n",
        "    save_inverted_index(inverted_index, inverted_index_file)\n",
        "\n",
        "    process_queries(preprocessed_path, inverted_index_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NqB1gtVSmqj",
        "outputId": "fdd55971-f373-4f72-b104-61863baa4928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-3025ac412035>:12: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, 'html.parser')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 2\n",
            "Enter query 1: Car bag in a canister\n",
            "Enter operations for query 1, separated by commas: OR, AND NOT\n",
            "Enter query 2: Coffee brewing techniques in cookbook\n",
            "Enter operations for query 2, separated by commas: AND, OR NOT, OR\n",
            "\n",
            "Query 1: car OR bag AND NOT canister\n",
            "Number of documents retrieved for query 1: 31\n",
            "Names of the documents retrieved for query 1: preprocessed_file682.txt, preprocessed_file686.txt, preprocessed_file118.txt, preprocessed_file698.txt, preprocessed_file166.txt, preprocessed_file313.txt, preprocessed_file363.txt, preprocessed_file3.txt, preprocessed_file404.txt, preprocessed_file542.txt, preprocessed_file956.txt, preprocessed_file780.txt, preprocessed_file942.txt, preprocessed_file864.txt, preprocessed_file930.txt, preprocessed_file860.txt, preprocessed_file174.txt, preprocessed_file264.txt, preprocessed_file573.txt, preprocessed_file886.txt, preprocessed_file797.txt, preprocessed_file466.txt, preprocessed_file665.txt, preprocessed_file892.txt, preprocessed_file746.txt, preprocessed_file738.txt, preprocessed_file699.txt, preprocessed_file863.txt, preprocessed_file981.txt, preprocessed_file73.txt, preprocessed_file459.txt\n",
            "\n",
            "Query 2: coffee AND brewing OR NOT techniques OR cookbook\n",
            "Number of documents retrieved for query 2: 0\n",
            "Names of the documents retrieved for query 2: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Positional Index and Phrase Queries"
      ],
      "metadata": {
        "id": "q0ftNjKYVtmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def default_dict():\n",
        "    return defaultdict(list)\n",
        "\n",
        "def create_positional_index(preprocessed_path):\n",
        "    positional_index = defaultdict(default_dict)\n",
        "    for file_name in os.listdir(preprocessed_path):\n",
        "        file_path = os.path.join(preprocessed_path, file_name)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            words = file.read().split()\n",
        "            for position, word in enumerate(words):\n",
        "                positional_index[word][file_name].append(position)\n",
        "    return positional_index\n",
        "\n",
        "\n",
        "def save_positional_index(positional_index, file_name):\n",
        "    with open(file_name, 'wb') as file:\n",
        "        pickle.dump(positional_index, file)\n",
        "\n",
        "def load_positional_index(file_name):\n",
        "    with open(file_name, 'rb') as file:\n",
        "        return pickle.load(file)\n",
        "\n",
        "def perform_phrase_query(positional_index, query_terms):\n",
        "    if not query_terms:\n",
        "        return set()\n",
        "\n",
        "    # Retrieving positional lists for each term in the phrase query\n",
        "    positional_lists = [positional_index[term] for term in query_terms]\n",
        "    all_docs = set.intersection(*map(set, positional_lists))\n",
        "\n",
        "    valid_docs = set()\n",
        "    for doc in all_docs:\n",
        "        positions = [positional_lists[i][doc] for i in range(len(query_terms))]\n",
        "        for pos in positions[0]:\n",
        "            if all(pos + i in positions[i] for i in range(len(query_terms))):\n",
        "                valid_docs.add(doc)\n",
        "                break\n",
        "\n",
        "    return valid_docs\n",
        "\n",
        "def process_queries(preprocessed_path, positional_index_file):\n",
        "    positional_index = load_positional_index(positional_index_file)\n",
        "    n = int(input(\"Enter the number of queries: \"))\n",
        "    for i in range(n):\n",
        "        query_text = input(f\"Enter phrase query {i + 1}: \")\n",
        "        preprocessed_query = preprocess_text(query_text)\n",
        "        query_terms = preprocessed_query.split()\n",
        "\n",
        "        result_docs = perform_phrase_query(positional_index, query_terms)\n",
        "        print(f\"Number of documents retrieved for query {i + 1} using positional index: {len(result_docs)}\")\n",
        "        print(f\"Names of documents retrieved for query {i + 1} using positional index: {', '.join(result_docs)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_path = \"/content/drive/My Drive/CSE508_Winter2024_A1_2021532/text_files\"\n",
        "    preprocessed_path = \"/content/drive/My Drive/CSE508_Winter2024_A1_2021532/Preprocessed_files\"\n",
        "    positional_index_file = \"/content/drive/My Drive/CSE508_Winter2024_A1_2021532/positional_index.pkl\"\n",
        "\n",
        "    preprocess_and_save_files(dataset_path, preprocessed_path)\n",
        "\n",
        "    positional_index = create_positional_index(preprocessed_path)\n",
        "    save_positional_index(positional_index, positional_index_file)\n",
        "\n",
        "    process_queries(preprocessed_path, positional_index_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgi4BRAQVv_f",
        "outputId": "212671e7-3f99-45c8-f3b1-7c15ede2f7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-3025ac412035>:12: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, 'html.parser')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 3\n",
            "Enter phrase query 1: it is a good in front for poutch\n",
            "Number of documents retrieved for query 1 using positional index: 0\n",
            "Names of documents retrieved for query 1 using positional index: \n",
            "Enter phrase query 2: it is good in reliable for fit\n",
            "Number of documents retrieved for query 2 using positional index: 1\n",
            "Names of documents retrieved for query 2 using positional index: preprocessed_file9.txt\n",
            "Enter phrase query 3: it is a fit front poutch\n",
            "Number of documents retrieved for query 3 using positional index: 1\n",
            "Names of documents retrieved for query 3 using positional index: preprocessed_file9.txt\n"
          ]
        }
      ]
    }
  ]
}